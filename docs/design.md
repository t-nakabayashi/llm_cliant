# LLMチャットWebアプリケーション設計書

## 要件定義書

### プロジェクトの目的
ollamaサーバーを駆動し、LLMのチャットを行うWebアプリケーションを開発する。ユーザーがブラウザ上でLLMとチャットできるインターフェースを提供し、ollamaの各種モデルを選択・設定できるようにする。

### 機能要件

#### ステップ1：基本チャット機能
- ブラウザにチャットウィンドウを表示
- ユーザーの入力をオウム返しする基本機能の実装
- シンプルなUIでのチャットインターフェース

#### ステップ2：ollamaモデル連携
- ollamaのモデル選択機能
- 起動画面の追加と実装
- 選択したモデルでのチャット機能

#### ステップ3：詳細設定
- ollamaモデルのコンテキスト長パラメータの変更機能
- その他のパラメータ設定オプション

### 非機能要件
- 使いやすいUI/UX
- レスポンス性能の最適化
- エラーハンドリングの実装
- セキュリティ対策

### 制約条件
- Pythonを使用したバックエンド実装
- Webアプリケーションとしての実装
- ollamaサーバーとの連携

## 設計書

### 概略設計

#### システム構成
- **バックエンド**：Flask（Pythonウェブフレームワーク）
- **フロントエンド**：HTML, CSS, JavaScript
- **ollamaとの連携**：ollamaのPython API（ollama-python）

#### 技術スタック
- **言語**：Python 3.9+
- **Webフレームワーク**：Flask
- **フロントエンド**：HTML, CSS, JavaScript（必要に応じてBootstrap）
- **通信**：WebSocket（Flask-SocketIO）
- **ollamaクライアント**：ollama-python

### 機能設計

#### チャットインターフェース
- チャット履歴の表示
- メッセージ入力フォーム
- 送信ボタン
- チャット履歴のスクロール機能

#### モデル選択機能
- 利用可能なollamaモデルのリスト表示
- モデル選択ドロップダウン
- モデル情報の表示

#### パラメータ設定機能
- コンテキスト長の設定スライダー
- 温度（temperature）設定
- トップP（top_p）設定
- その他のパラメータ設定オプション

### クラス構成

#### `app.py`（メインアプリケーション）
- Flaskアプリケーションの初期化と設定
- ルーティング設定
- WebSocketイベントハンドラ

#### `ollama_client.py`
- `OllamaClient`クラス：ollamaサーバーとの通信を担当
  - モデル一覧取得
  - チャット実行
  - パラメータ設定

#### `chat_session.py`
- `ChatSession`クラス：チャットセッションの管理
  - メッセージ履歴の保持
  - コンテキスト管理
  - セッション設定

#### `static/js/chat.js`
- フロントエンドのチャット機能実装
- WebSocket通信
- UIの動的更新

#### `templates/`
- `index.html`：メインページ
- `chat.html`：チャットインターフェース
- `settings.html`：設定ページ

### データフロー
1. ユーザーがブラウザでアプリケーションにアクセス
2. フロントエンドがバックエンドからモデル一覧を取得
3. ユーザーがモデルを選択し、チャットを開始
4. ユーザーの入力がWebSocketを通じてバックエンドに送信
5. バックエンドがollamaサーバーにリクエストを送信
6. ollamaサーバーからのレスポンスをユーザーに返す

### 開発ステップ

#### ステップ1：基本チャット機能
1. Flaskアプリケーションの基本構造を実装
2. シンプルなチャットUIの実装
3. WebSocketによるリアルタイム通信の実装
4. オウム返し機能の実装

#### ステップ2：ollamaモデル連携
1. ollama-pythonクライアントの実装
2. モデル一覧取得機能の実装
3. モデル選択UIの実装
4. 選択したモデルでのチャット機能の実装

#### ステップ3：詳細設定
1. パラメータ設定UIの実装
2. コンテキスト長設定機能の実装
3. その他のパラメータ設定機能の実装
4. 設定の保存と読み込み機能の実装